{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7b107958-252f-4390-a27e-35e49dcc7284",
   "metadata": {},
   "source": [
    "# N-Gram Text Correction using Natural Language Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9b08159-038b-42ef-8948-74a44abfb562",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "Developed two N-gram language models, a bigram and a fivegram, to predict missing words in a text. The sample text utilized to train and evaluate these models is the work of William Shakespeare.\n",
    "\n",
    "**Tasks:**\n",
    "1. Data Preparation\n",
    "2. N-gram Model Training\n",
    "3. Text Correction\n",
    "4. Evaluation\n",
    "5. Export Models\n",
    "\n",
    "**Files used:**\n",
    "\n",
    "WS_train.txt - All WS works.\r\n",
    "\r\n",
    "WS_test.txt - The manuscript, with the words lost marked as .\r\n",
    "\r\n",
    "WS_validation - Text to validade our models performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db0d2a3d-635d-4be3-9d85-3bca416e3c7a",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "68c78f81-ec6f-4864-8289-a12ac27eafe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string, nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.util import ngrams"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c2c938d-7297-48d3-a5df-1a8bd900ea03",
   "metadata": {},
   "source": [
    "### Load Text Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "53c75071-9e04-44d3-895a-195632c57459",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_text(file_path):\n",
    "    text = open(file_path, encoding = 'utf-8').read().lower()\n",
    "    return text\n",
    "\n",
    "train_text = load_text('./WS_train.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fd647fef-2dc7-46eb-8ae4-020fb301d8f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    punct = string.punctuation\n",
    "    stop = stopwords.words('english')\n",
    "    tokens = word_tokenize(text)\n",
    "    cleaned = []\n",
    "    for word in tokens:\n",
    "        if  word == 'deleted' or word == 'DELETED' or word == '<DELETED>' or word == '<deleted>':\n",
    "            cleaned.append('<DELETED>')\n",
    "        else:\n",
    "            if word.lower() not in punct:\n",
    "                if word.lower() not in stop:\n",
    "                    cleaned.append(word.lower())\n",
    "    return cleaned\n",
    "\n",
    "cleaned_train_text = clean_text(train_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9f17cc28-6111-4698-963a-3b20e5bf5a80",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_ngrams(tokens, n):\n",
    "    new = []\n",
    "    grams = []\n",
    "    padded = []\n",
    "    i = 1\n",
    "\n",
    "    while i < n:\n",
    "        padded.append('<s>')\n",
    "        i += 1\n",
    "\n",
    "    padded = (padded + tokens)\n",
    "    padded.append('</s>')\n",
    "    grams = list(ngrams(padded, n))\n",
    "    return grams\n",
    "\n",
    "train_bigrams = create_ngrams(cleaned_train_text, 2)\n",
    "train_fivegrams = create_ngrams(cleaned_train_text, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "44e9a580-0538-4608-9780-35c210d46b73",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vocab(tokens):\n",
    "    vocab = []\n",
    "    for x in tokens:\n",
    "        if x != '<DELETED>':\n",
    "            vocab.append(x)\n",
    "    return set(vocab)\n",
    "\n",
    "vocab = build_vocab(cleaned_train_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "90df34c0-b844-4c5f-bfb2-d721466c9f04",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.probability import FreqDist, ConditionalFreqDist, ConditionalProbDist, MLEProbDist\n",
    "\n",
    "def calculate_ngram_freq(ngrams_list):\n",
    "    freq = FreqDist(ngrams_list)\n",
    "    return freq\n",
    "\n",
    "bigram_freq_dist = calculate_ngram_freq(train_bigrams)\n",
    "fivegram_freq_dist = calculate_ngram_freq(train_fivegrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8deb72c1-2649-4647-ba27-60925a9ad678",
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_ngram_probabilities(ngrams_list):\n",
    "    new = []\n",
    "    for y in ngrams_list:\n",
    "        first = y[:-1]\n",
    "        last = y[-1]\n",
    "        new.append((first, last))\n",
    "    confreq = ConditionalFreqDist(new)\n",
    "    x = ConditionalProbDist(confreq, MLEProbDist, 50000)\n",
    "    return x\n",
    "\n",
    "bigram_prob_dist = estimate_ngram_probabilities(train_bigrams)\n",
    "fivegram_prob_dist = estimate_ngram_probabilities(train_fivegrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e047660d-ac90-44e4-94c0-9c09a76e52d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_next_word(context, cpd, top_n=1):\n",
    "    if context not in cpd:\n",
    "        return ['<UNK>']\n",
    "    else:\n",
    "        prob1 = []\n",
    "        cond_dist = cpd[context]\n",
    "        for y in cond_dist.samples():\n",
    "            prob1.append((y, cond_dist.prob(y)))\n",
    "        top_prob = sorted(prob1, key = lambda x: x[1], reverse = True)\n",
    "        return [word for word, prob in top_prob[:top_n]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1053abae-2396-488f-9915-91edac267d35",
   "metadata": {},
   "outputs": [],
   "source": [
    "def correct_text_with_ngrams(text_data, cpd, n):\n",
    "    corrected_text = []\n",
    "    for i in range(len(text_data)):\n",
    "        word = text_data[i]\n",
    "        if word == '<DELETED>':\n",
    "            context = tuple(text_data[max(0, i-(n-1)):i])\n",
    "            pred_word = predict_next_word(context, cpd, n)\n",
    "            corrected_text.append(pred_word[0])\n",
    "        else:\n",
    "            corrected_text.append(word)\n",
    "    return corrected_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "64848f5b-dad5-4614-a6af-6aca7b1f43ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_text = load_text('./WS_test.txt')\n",
    "cleaned_test_text = clean_text(test_text)\n",
    "corrected_test_text_bigram = correct_text_with_ngrams(cleaned_test_text, bigram_prob_dist, 2)\n",
    "corrected_test_text_fivegram = correct_text_with_ngrams(cleaned_test_text, fivegram_prob_dist, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4d3a1a20-4b68-4e45-ab7a-1507de295ed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_accuracy(test_tokens, corrected_tokens, validation_tokens):\n",
    "    if (len(test_tokens) != len(corrected_tokens)) or (len(test_tokens) != len(validation_tokens)):\n",
    "        print(\"Test Tokens, Validation Token and Corrected Tokens must have the same length\")\n",
    "        return\n",
    "    \n",
    "    correct_predictions = 0\n",
    "    all_predictions = 0 \n",
    "    for i in range(len(test_tokens)):\n",
    "        if test_tokens[i] == '<DELETED>':\n",
    "            all_predictions+=1\n",
    "            if corrected_tokens[i] == validation_tokens[i]:\n",
    "                   correct_predictions+=1\n",
    "                    \n",
    "    return correct_predictions/all_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4618624f-653c-4dd4-bd68-975a225d06e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_text = load_text('./WS_validation.txt')\n",
    "cleaned_validation_text = clean_text(validation_text)\n",
    "bigram_accuracy = calculate_accuracy(cleaned_test_text, corrected_test_text_bigram, cleaned_validation_text)\n",
    "fivegram_accuracy = calculate_accuracy(cleaned_test_text, corrected_test_text_fivegram, cleaned_validation_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ce828fe6-ca9c-4f02-8f30-862875622350",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('fivegram_prob_dist.pkl', 'wb') as file:\n",
    "    pickle.dump(fivegram_prob_dist , file)\n",
    "\n",
    "with open('bigram_prob_dist.pkl', 'wb') as file:\n",
    "    pickle.dump(bigram_prob_dist , file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae417f84-009e-4e6c-98ee-64c855cadbcf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
